{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4026afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supervised learning: feature is given to us and label is given to us.\n",
    "# in general, we find relationship between features and labels,,,\n",
    "# In general, we do such things in case of supervised learning approach...\n",
    "# In supervised learning, we have mainly classification and regression problems...\n",
    "# that can be solved by different types of algos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7333923e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. K_mean\\n2. Hierarchical clustering\\n3. DB_Scan\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unsupervised Learning: In case of unsupervised learning, we are not supposed to provide labels at all.\n",
    "# Whole dataset is considered as features itself.\n",
    "# i.e. entire data is given as training data where no labels are given\n",
    "\n",
    "# clustering is like grouping of similar data,,,\n",
    "# the only difference is that clustering is a trainable approach, on the other hand, group is like if_and_else-like approach.\n",
    "\n",
    "\n",
    "#Why is clustering required???\n",
    "# we create multiple groups, each having similar type of data,,,i.e. having similar variance...similar nature\n",
    "# and then training is done...Thus we are able to get best accuracy...this where unsupervised learning and clustering comes into picture.\n",
    "\n",
    "# NOTE: KNN is a classification and regression algo, whereas K-mean is a clustering algo\n",
    "# KNN follows supervised ML approach whereas K-mean follows unsupervised ML approach\n",
    "# In unsupervised learning, we will learning following algos:\n",
    "\"\"\"\n",
    "1. K_mean\n",
    "2. Hierarchical clustering\n",
    "3. DB_Scan\n",
    "\"\"\"\n",
    "\n",
    "# Clustering application: Customer segmentation, resume selection, data analysis, \n",
    "# anomaly detection, search engine, speech/ image segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b49e0",
   "metadata": {},
   "source": [
    "# K-Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "917ab91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a. \n",
    "\n",
    "# This algo will first of all try to find out the optimal value of K\n",
    "# But the way of finding out this vaue of K is different,\n",
    "# i.e. by using an approach called WCSS (Within Cluster Summation of Square)\n",
    "# Ex. - If K = 2 ,,,it means we have chosen 2 points (2 centroid coordinates)\n",
    "# The points from K-value searches for nearest datapoint to associated with it to form a cluster\n",
    "# After association with nearest point(from given dataset), centroid is shifted to a new position\n",
    "# And thus on every association, a new centroid is formed.\n",
    "# Finally, groups/ clusters are formed with a finalized centroid\n",
    "# And whenever a new dataset is found, it always try to find out the nearest cluster to associate with it\n",
    "# wcss is nothing but sum of squares of all distances (i.e. b/w each points and its coordinate) \n",
    "# SS = SS1 + SS2 = summation of((c1-pi)^2) + summation of((c2-pi)^2), if k = 2\n",
    "# or wcss = double summtion of (ci - pi)^2...this called inertia.\n",
    "# In case of k = n, each and every points will behave as a centroid,\n",
    "# and Summaation of square = 0 and therefore, wcss = 0 in case of k = n.\n",
    "# So, we can say that in case of k = 1, the distance will be maximum, and would further decrease as the value of k increases\n",
    "# i.e. wcss or inertia would tend to zero as the value of k increases.\n",
    "# ELBOW PLOT ia graph or plot for k vs wcss/inertia\n",
    "# and through this graph we would try to find out the dispersion point on the graph,\n",
    "# the point after which major changes are not visible on the plotted graph\n",
    "# the value of k corresponding to the dispersion point will give the value of k\n",
    "# once the value of k is decided, k number of random centroids are created/ generated and then association starts\n",
    "\n",
    "# the major drawback of K-mean clustering algo is the allocation of centroid\n",
    "# the random generation of centroids doesn't guarantee best position of centroids,,,\n",
    "# e.g. - it may be possible that all of the generated centroids would be near to each other and after creation of one cluster, \n",
    "# it may be possible that the other centroids would not be able to find the nearest data and \n",
    "# hence would not be able to participate in clustering.\n",
    "\n",
    "#b. \n",
    "\n",
    "# IN k-mean ++, some minor improvement was done. The distance b/w the generated centroids should be certain \n",
    "# so that they would not come close together.\n",
    "# Later on, triangle rule was followed to reduce no. of calculations for distance...a+b > c\n",
    "\n",
    "#c. \n",
    "\n",
    "# later on mini batch k-mean was introduced to not to provide whole data, but some data in batches\n",
    "\n",
    "# NOTE: by default, when we use k-mean algo, we use k-mean++ version of the k-mean algo as it is default\n",
    "\n",
    "#PARAMETERS under k-mean:\n",
    "\n",
    "# 1. n_init: Number of times k-mean algos will run. The final selection would be the best of all these n algos\n",
    "# 2. tol: tune the model in a little bit better way\n",
    "# 3. Read on your own for better understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1328ea71",
   "metadata": {},
   "source": [
    "# Hierarchical clustering or agglomerative approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ebf8e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whenever we try to build a cluster, we follow either \n",
    "# agglomerative approach or divisive approach.\n",
    "\n",
    "# divisive is nothing but k-mean algo approach, where we are trying to consider all the points as a part of one single cluster, \n",
    "# and then we keep dividing data into different small clusters \n",
    "\n",
    "# in case of agglomerative approach, we always start with creating n number of clusters for n number of data points. ANd then we keep on combining certain data points based on some rules and regulisations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1efe1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The problem with k-mean algo is that we are supposed to know the value of k before hand.\n",
    "\n",
    "# Hierarchical clustering: (Bottom-up approach): We try to consider n number of data points as individual clusters and then based on similarity, we follow agglomerative approach to keep adding...n then n-1,,,,then n-2 ...therefore bottom up approach\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
